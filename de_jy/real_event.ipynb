{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_text(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìžˆëŠ” iframe ì•ˆ ë§í¬ ì°¾ê¸°\n",
    "        iframe = soup.find('iframe')\n",
    "        if not iframe:\n",
    "            return ''\n",
    "        iframe_url = \"https://dart.fss.or.kr\" + iframe['src']\n",
    "        \n",
    "        res = requests.get(iframe_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
    "        iframe_soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        text = iframe_soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_events_by_similarity(texts, threshold=0.75):\n",
    "    # stop_words='korean' ì œê±° â†’ ì˜¤ë¥˜ í•´ê²°\n",
    "    vectorizer = TfidfVectorizer(stop_words=None, max_df=0.8)\n",
    "    tfidf = vectorizer.fit_transform(texts)\n",
    "    sim_matrix = cosine_similarity(tfidf)\n",
    "\n",
    "    n = len(texts)\n",
    "    groups = [-1] * n\n",
    "    group_id = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if groups[i] == -1:\n",
    "            groups[i] = group_id\n",
    "            for j in range(i + 1, n):\n",
    "                if sim_matrix[i][j] >= threshold:\n",
    "                    groups[j] = group_id\n",
    "            group_id += 1\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:04<00:00, 16.47it/s]\n",
      "ê¸°ì—…ë³„ ì‚¬ê±´ ê·¸ë£¹í•‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 1368.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv('data/merged_event_final.csv')\n",
    "\n",
    "# ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘\n",
    "df['text'] = df['source'].progress_apply(get_report_text)\n",
    "\n",
    "# ê¸°ì—…ë³„ ì‚¬ê±´ ê·¸ë£¹í•‘\n",
    "df['event_id'] = None\n",
    "\n",
    "for corp_id in tqdm(df['company_corp_id'].unique(), desc=\"ê¸°ì—…ë³„ ì‚¬ê±´ ê·¸ë£¹í•‘\"):\n",
    "    sub_df = df[df['company_corp_id'] == corp_id].copy()\n",
    "\n",
    "    if len(sub_df) == 1:\n",
    "        df.loc[sub_df.index, 'event_id'] = f\"{corp_id}_evt_0\"\n",
    "        continue\n",
    "\n",
    "    texts = sub_df['text'].fillna('').tolist()\n",
    "\n",
    "    # ðŸ”» ì¶”ê°€: ëª¨ë‘ ë¹ˆ í…ìŠ¤íŠ¸ë©´ ê±´ë„ˆë›°ê¸°\n",
    "    if all(len(text.strip()) == 0 for text in texts):\n",
    "        df.loc[sub_df.index, 'event_id'] = f\"{corp_id}_evt_999\"  # ìž„ì‹œ event_id\n",
    "        continue\n",
    "\n",
    "    groups = group_events_by_similarity(texts, threshold=0.75)\n",
    "\n",
    "    for idx, g in zip(sub_df.index, groups):\n",
    "        df.loc[idx, 'event_id'] = f\"{corp_id}_evt_{g}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¶„ë¥˜ ì™„ë£Œ! 'classified_events.csv', 'company_event_summary.csv' ìƒì„±ë¨.\n"
     ]
    }
   ],
   "source": [
    "# ì €ìž¥\n",
    "df.to_csv('data/classified_events.csv', index=False)\n",
    "\n",
    "# ê¸°ì—…ë³„ ì‚¬ê±´ ìˆ˜ ìš”ì•½\n",
    "event_counts = df[['company_corp_id', 'event_id']].drop_duplicates()\n",
    "summary = event_counts['company_corp_id'].value_counts().reset_index()\n",
    "summary.columns = ['company_corp_id', 'unique_event_count']\n",
    "summary.to_csv('company_event_summary.csv', index=False)\n",
    "\n",
    "print(\"âœ… ë¶„ë¥˜ ì™„ë£Œ! 'classified_events.csv', 'company_event_summary.csv' ìƒì„±ë¨.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
